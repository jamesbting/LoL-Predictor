{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cy7hD4XlgUf",
        "colab_type": "code",
        "outputId": "1d12e55f-bbc8-42c9-efa4-c31afd951ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#setup the environment for the NN, depending on if it to be run on Google Colab, or a local device (with or without a CUDA-enable GPU)\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "\n",
        "#OPTIONS\n",
        "REBUILD_DATA = True # set to true to one once, then back to false unless you want to change something in your training data.\n",
        "GOOGLE_COLAB = False #set to true if running in google colab, false other wise\n",
        "INCLUDE_POST_MATCH = False #set to true if you want to include the post match data\n",
        "SAVE_GRAPHS = False # set to true if you want to save the graphs for later examination\n",
        "SAVE_MODEL = False #set to true if you want to save the model for later use\n",
        "\n",
        "#set up environment\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")\n",
        "\n",
        "if GOOGLE_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    training_data_filename = \"/content/gdrive/My Drive/Colab Notebooks/LoL-Predictor/datasets/training_data.csv\" #for running on google colab notebooks\n",
        "    model_log_filename = \"/content/gdrive/My Drive/Colab Notebooks/LoL-Predictor/logs/model.log\"\n",
        "    numpy_training_data = \"/content/gdrive/My Drive/Colab Notebooks/LoL-Predictor/datasets/training_data.npy\"\n",
        "    model_save_location = \"/content/gdrive/My Drive/Colab Notebooks/LoL-Predictor/models\"\n",
        "    if INCLUDE_POST_MATCH:\n",
        "        excluded_columns_file = \"/content/gdrive/My Drive/Colab Notebooks/LoL-Predictor/docs/PostMatchExcludedColumns.csv\"\n",
        "    else:\n",
        "         excluded_columns_file = \"/content/gdrive/My Drive/Colab Notebooks/LoL-Predictor/docs/PreMatchExcludedColumns.csv\"\n",
        "else:\n",
        "    training_data_filename = '..\\\\datasets\\\\training_data.csv'\n",
        "    model_log_filename = '..\\\\logs\\\\model.log'\n",
        "    numpy_training_data = '..\\\\datasets\\\\training_data.npy'\n",
        "    model_save_location = '..\\\\models'\n",
        "    if INCLUDE_POST_MATCH:\n",
        "        excluded_columns_file = '..\\\\docs\\\\PostMatchExcludedColumns.csv'\n",
        "    else:\n",
        "        excluded_columns_file = \"..\\\\docs\\\\PreMatchExcludedColumns.csv\"\n",
        "        "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Running on the CPU\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBCn0RUBlgUn",
        "colab_type": "code",
        "tags": [
          "outputPrepend"
        ],
        "colab": {}
      },
      "source": [
        "#class that reads data from the file, and prepares the training data array\n",
        "class DataSetReader():\n",
        "    LABELS = {'blue_win':[1,0] , 'red_win':[0,1]}\n",
        "    training_data = []\n",
        "    blue_win_count = 0\n",
        "    red_win_count = 0\n",
        "    training_data_location = training_data_filename\n",
        "    exclude_columns_indices = None\n",
        "    with open(excluded_columns_file,\"r\") as f:\n",
        "      reader = csv.reader(f)\n",
        "      exclude_columns_indices = tuple(next(reader))\n",
        "      \n",
        "\n",
        "    def read_training_data(self):\n",
        "      with open(self.training_data_location,'r') as f:\n",
        "        next(f) #skip the line with the column names\n",
        "        for row in f:\n",
        "          matchData = self.read_and_exclude_row(row,DataSetReader.exclude_columns_indices)\n",
        "          result = None\n",
        "          if(matchData[-1] == 1): #red win, blue loos\n",
        "            result = self.LABELS[\"red_win\"]\n",
        "            self.red_win_count += 1\n",
        "          elif (matchData[-1] == 0): #red loss, blue win\n",
        "            result = self.LABELS[\"blue_win\"]\n",
        "            self.blue_win_count += 1\n",
        "          else:\n",
        "            raise TypeError\n",
        "          self.training_data.append([np.array(matchData[:-2]),result])\n",
        "      f.close()\n",
        "      np.random.shuffle(self.training_data)\n",
        "      np.save(numpy_training_data, self.training_data)\n",
        "      print('Blue Win Count:',self.blue_win_count)\n",
        "      print('Red Win Count:', self.red_win_count)\n",
        "          \n",
        "          \n",
        "\n",
        "    def read_and_exclude_row(self,row,exclude_column_indices):\n",
        "        row_list = row.split(',')\n",
        "        cleaned_row = []\n",
        "        append = cleaned_row.append\n",
        "        for index,value in enumerate(row_list):\n",
        "            if str(index) not in exclude_column_indices:\n",
        "              if value == 'FALSE' or value == 'False':\n",
        "                append(0)\n",
        "              elif value == 'TRUE' or value == 'True':\n",
        "                append(1)\n",
        "              else:\n",
        "                append(float(value))\n",
        "        return cleaned_row\n",
        "\n",
        "    def get_input_size(self):\n",
        "      return len(self.training_data[0][0])\n",
        "    \n",
        "    def get_training_set(self):\n",
        "      return self.training_data\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG2CuJV1VpnR",
        "colab_type": "code",
        "outputId": "485c530e-4b28-4fb8-d997-f49e04a1d046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "if REBUILD_DATA:\n",
        "  data_reader = DataSetReader()\n",
        "  data_reader.read_training_data()\n",
        "  INPUT_SIZE = data_reader.get_input_size()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Blue Win Count: 4974\nRed Win Count: 5039\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQrThQQ2Vpnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    #constructor\n",
        "    def __init__(self,layer_size,dropout_rate):\n",
        "        super().__init__() #superclass constructor\n",
        "        self.fc1 = nn.Linear(INPUT_SIZE,layer_size)\n",
        "        self.fc2 = nn.Linear(layer_size,layer_size)\n",
        "        self.fc3 = nn.Linear(layer_size,2)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.batchnorm = nn.BatchNorm1d(layer_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.batchnorm(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "   "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFnewqvrVpns",
        "colab_type": "code",
        "outputId": "2cdccd2d-f50d-4d6f-ba56-824b3d36ce8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainset = np.load(numpy_training_data,allow_pickle=True)\n",
        "print(\"Size of data set:\",len(trainset))\n",
        "\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "X = torch.Tensor([i[0] for i in trainset])\n",
        "X = torch.log10(X + torch.ones(INPUT_SIZE)) #normalize the data to put it through a log(x+1) function\n",
        "y = torch.Tensor([i[1] for i in trainset])\n",
        "\n",
        "VAL_PERCENT = 0.1\n",
        "val_size = int(len(X)*VAL_PERCENT)\n",
        "\n",
        "train_X = X[:-val_size]\n",
        "train_y = y[:-val_size]\n",
        "test_X = X[-val_size:]\n",
        "test_y = y[-val_size:]\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Size of data set: 10013\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSmau5jfVpn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_pass(X,y,train = False):\n",
        "    if train:\n",
        "        optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    matches = [torch.argmax(i,0) == torch.argmax(j,0) for i,j in zip(outputs,y)]\n",
        "    acc = matches.count(True)/len(matches)\n",
        "    cross_entropy_loss = loss_function(outputs,y)\n",
        "\n",
        "    all_linear1_params = torch.cat([x.view(-1) for x in net.fc1.parameters()])\n",
        "    all_linear2_params = torch.cat([x.view(-1) for x in net.fc2.parameters()])\n",
        "    l1_regularization = lambda1 * torch.norm(all_linear1_params, 1)\n",
        "    l2_regularization = lambda2 * torch.norm(all_linear2_params, 2)\n",
        "\n",
        "    loss = cross_entropy_loss + l1_regularization + l2_regularization\n",
        "\n",
        "    if train:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return acc, loss\n",
        "\n",
        "def test(size=32):\n",
        "    X, y = test_X[:size], test_y[:size]\n",
        "    val_acc, val_loss = forward_pass(X.view(-1,INPUT_SIZE).to(device), y.to(device).view(-1,2))\n",
        "    return val_acc, val_loss\n",
        "\n",
        "def train(net,model_name):\n",
        "    with open(model_log_filename,\"a+\") as f:\n",
        "        for epoch in range(EPOCHS):\n",
        "                for i in tqdm(range(0, len(train_X), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:100] ..for now just to dev\n",
        "                    batch_X = train_X[i:i+BATCH_SIZE].view(-1,INPUT_SIZE)\n",
        "                    batch_y = train_y[i:i+BATCH_SIZE].view(-1,2)\n",
        "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "                    acc, loss = forward_pass(batch_X, batch_y, train=True)\n",
        "                    if i % BATCH_SIZE == 0:\n",
        "                        val_acc, val_loss = test(size=BATCH_SIZE)\n",
        "                        f.write(f\"{model_name},{round(float(time.time()),3)},{round(float(acc),2)},{round(float(loss), 4)},{round(float(val_acc),2)},{round(float(val_loss),4)}\\n\")\n",
        "\n",
        "\n",
        "def create_acc_loss_graph(model_name,save_graphs=False):\n",
        "    contents = open(model_log_filename,\"r\").read().split(\"\\n\")\n",
        "    times = []\n",
        "    accuracies = []\n",
        "    losses = []\n",
        "    val_accs = []\n",
        "    val_losses = []\n",
        "\n",
        "    for c in contents:\n",
        "        if model_name in c:\n",
        "            name, timestamp, acc, loss, val_acc, val_loss = c.split(\",\")\n",
        "            times.append(timestamp)\n",
        "            accuracies.append(acc)\n",
        "            losses.append(loss)\n",
        "\n",
        "            val_accs.append(float(val_acc))\n",
        "            val_losses.append(float(val_loss))\n",
        "\n",
        "    #accuracy graph\n",
        "    fog = plt.figure()\n",
        "    ax1 = plt.subplot2grid((2,1),(0,0))\n",
        "    ax2 = plt.subplot2grid((2,1), (1,0), sharex=ax1)\n",
        "\n",
        "    ax1.plot(times, accuracies, label=\"test_acc\",color=\"red\")\n",
        "    ax1.axes.get_xaxis().set_ticks([])\n",
        "    ax1.legend(loc=2)\n",
        "    ax1.set_title(\"Training and Valdiation Accuracy\")\n",
        "   \n",
        "    ax2.plot(times,val_accs, label=\"val_acc\")\n",
        "    ax2.legend(loc=2)\n",
        "  \n",
        "\n",
        "    if save_graphs:\n",
        "        plt.savefig(f\"../logs/graphs/{model_name}-POSTMATCHDATA-{INCLUDE_POST_MATCH}-acccuracies.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    #loss graph\n",
        "    fog = plt.figure()\n",
        "    ax1 = plt.subplot2grid((2,1),(0,0))\n",
        "    ax2 = plt.subplot2grid((2,1), (1,0), sharex=ax1)\n",
        "    \n",
        "    ax1.plot(times,losses, label=\"test_loss\",color=\"red\")\n",
        "    ax1.axes.get_xaxis().set_ticks([])\n",
        "    ax1.legend(loc=2)\n",
        "    ax1.set_title(\"Training and Validation Loss\")\n",
        "    \n",
        "    ax2.plot(times,val_losses, label=\"val_loss\")\n",
        "    ax2.legend(loc=2)\n",
        "    \n",
        "    if save_graphs:\n",
        "         plt.savefig(f\"../logs/graphs/{model_name}-POSTMATCHDATA-{INCLUDE_POST_MATCH}-losses.png\")\n",
        "    plt.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#function to save models\n",
        " def save_model(net,model_name):\n",
        "        filename = model_save_location + \"\\\\\" + model_name\n",
        "        torch.save(net.state_dict(),filename)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_heOdUMVpoD",
        "colab_type": "code",
        "outputId": "9e380195-847f-4784-f2aa-f3a5f95d7c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "layer_sizes = [32,64,128,256,512,1024,2048]\n",
        "lambda1, lambda2 = 0.5, 0.01\n",
        "if INCLUDE_POST_MATCH:\n",
        "      learning_rate = 0.001\n",
        "      dropout_rate = 0.1\n",
        "else:\n",
        "      learning_rate = 0.0005\n",
        "      dropout_rate = 0.2\n",
        "\n",
        "for layer_size in layer_sizes:\n",
        "      net = Net(layer_size,dropout_rate).to(device)\n",
        "      optimizer = optim.Adam(net.parameters(),lr = learning_rate)\n",
        "      model_name = f\"model-layersize{layer_size}-{int(time.time())}\"\n",
        "      train(net,model_name)\n",
        "      print(\"Hidden Layer size of:\",layer_size)\n",
        "      #make the graphs\n",
        "      create_acc_loss_graph(model_name,SAVE_GRAPHS)\n",
        "\n",
        "      #save the model if desired\n",
        "      if SAVE_MODEL:\n",
        "            save_model(net,model_name)\n",
        "            print(\"Model Saved!\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "0%|          | 0/46 [00:00<?, ?it/s]\n"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lambda1' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-8-bd3fb92c662d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m       \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m       \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"model-layersize{layer_size}-{int(time.time())}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m       \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Hidden Layer size of:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlayer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m       \u001b[1;31m#make the graphs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-6-5be3ab1dd99d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, model_name)\u001b[0m\n\u001b[0;32m     32\u001b[0m                     \u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                     \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                         \u001b[0mval_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-6-5be3ab1dd99d>\u001b[0m in \u001b[0;36mforward_pass\u001b[1;34m(X, y, train)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mall_linear1_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mall_linear2_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0ml1_regularization\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlambda1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_linear1_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0ml2_regularization\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlambda2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_linear2_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'lambda1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hpzmyqSVpoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [],
      "execution_count": 0,
      "outputs": []
    }
  ],
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python36864bitf498ff391bd141efa82a8b19ebf9c273",
      "display_name": "Python 3.6.8 64-bit"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}