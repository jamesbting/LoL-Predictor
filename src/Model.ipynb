{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cy7hD4XlgUf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2179c16e-ab4c-4a72-d8e2-2bb8e861d6f4"
      },
      "source": [
        "#setup the environment for the NN, depending on if it to be run on Google Colab, or a local device (with or without a CUDA-enable GPU)\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "#set up environment\n",
        "REBUILD_DATA = True # set to true to one once, then back to false unless you want to change something in your training data.\n",
        "GOOGLE_COLAB = False #set to true if running in google colab, false other wise\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")\n",
        "\n",
        "if GOOGLE_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    training_data_filename = \"/content/gdrive/My Drive/Colab Notebooks/LoL-Predictor/datasets/training_data.csv\" #for running on google colab notebooks\n",
        "    model_log_filename = \"/content/gdrive/My Drive/Colab Notebooks/LoL-Predictor/logs/model.log\"\n",
        "    numpy_training_data = \"/content/gdrive/My Drive/Colab Notebooks/LoL-Predictor/datasets/training_data.npy\"\n",
        "else:\n",
        "    training_data_filename = \"C:\\\\Users\\\\James Ting\\\\OneDrive - McGill University\\\\Personal\\\\Personal Projects\\\\LoL-Predictor\\\\datasets\\\\training_data.csv\"\n",
        "    model_log_filename = \"C:\\\\Users\\\\James Ting\\\\OneDrive - McGill University\\\\Personal\\\\Personal Projects\\\\LoL-Predictor\\\\logs\\\\model.log\"\n",
        "    numpy_training_data = \"C:\\\\Users\\\\James Ting\\\\OneDrive - McGill University\\\\Personal\\\\Personal Projects\\\\LoL-Predictor\\\\datasets\\\\training_data.npy\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Running on the CPU\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBCn0RUBlgUn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ccb71bea-0c23-406a-d1bf-b6f088cde3fd",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "\n",
        "class DataSetReader():\n",
        "    LABELS = {'blue_win':[1,0] , 'red_win':[0,1]}\n",
        "    training_data = []\n",
        "    blue_win_count = 0\n",
        "    red_win_count = 0\n",
        "    training_data_location = training_data_filename\n",
        "    exclude_columns = (0, 23, 25, 26, 29, 98, 100, 101, 104, 173, 175, 176, \n",
        "                       179, 248, 250, 251, 254, 323, 325, 326, 329, 398, 400, \n",
        "                       401, 404, 473, 475, 476, 479, 548, 550, 551, 554, 623, \n",
        "                       625, 626, 629, 698, 700, 701, 704)\n",
        "\n",
        "\n",
        "    def read_training_data(self):\n",
        "      with open(self.training_data_location,'r') as f:\n",
        "        next(f) #skip the line with the column names\n",
        "        for row in f:\n",
        "          matchData = self.read_and_exclude_row(row,self.exclude_columns)\n",
        "          result = None\n",
        "          if(matchData[-1] == 1): #red win, blue loos\n",
        "            result = self.LABELS[\"red_win\"]\n",
        "            self.red_win_count += 1\n",
        "          elif (matchData[-1] == 0): #red loss, blue win\n",
        "            result = self.LABELS[\"blue_win\"]\n",
        "            self.blue_win_count += 1\n",
        "          else:\n",
        "            raise TypeError\n",
        "          self.training_data.append([np.array(matchData[:-2]),result])\n",
        "      \n",
        "      \n",
        "      f.close()\n",
        "      np.random.shuffle(self.training_data)\n",
        "      np.save(numpy_training_data, self.training_data)\n",
        "      print('Blue Win Count:',self.blue_win_count)\n",
        "      print('Red Win Count:', self.red_win_count)\n",
        "          \n",
        "          \n",
        "\n",
        "    def read_and_exclude_row(self,row,exclude_column_indices):\n",
        "        row_list = row.split(',')\n",
        "        cleaned_row = []\n",
        "        append = cleaned_row.append\n",
        "        for index,value in enumerate(row_list):\n",
        "            if index not in exclude_column_indices:\n",
        "              if value == 'FALSE' or value == 'False':\n",
        "                append(0)\n",
        "              elif value == 'TRUE' or value == 'True':\n",
        "                append(1)\n",
        "              else:\n",
        "                append(float(value))\n",
        "        return cleaned_row\n",
        "\n",
        "    def get_input_size(self):\n",
        "      return len(self.training_data[0][0])\n",
        "    \n",
        "    def get_training_set(self):\n",
        "      return self.training_data\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Blue Win Count: 1638\nRed Win Count: 1689\n"
        }
      ],
      "source": [
        "if REBUILD_DATA:\n",
        "  data_reader = DataSetReader()\n",
        "  data_reader.read_training_data()\n",
        "  INPUT_SIZE = data_reader.get_input_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    #constructor\n",
        "    def __init__(self,layer_size):\n",
        "        super().__init__() #superclass constructor\n",
        "        self.fc1 = nn.Linear(INPUT_SIZE,layer_size)\n",
        "        self.fc2 = nn.Linear(layer_size,layer_size)\n",
        "        self.fc3 = nn.Linear(layer_size,2)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "        self.batchnorm = nn.BatchNorm1d(layer_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.batchnorm(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Size of data set:  3327\n"
        }
      ],
      "source": [
        "\n",
        "trainset = np.load(numpy_training_data,allow_pickle=True)\n",
        "print(\"Size of data set: \",len(trainset))\n",
        "\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "X = torch.Tensor([i[0] for i in trainset])\n",
        "X = torch.log10(X + torch.ones(INPUT_SIZE)) #normalize the data to put it through a log(x+1) function\n",
        "y = torch.Tensor([i[1] for i in trainset])\n",
        "\n",
        "VAL_PERCENT = 0.1\n",
        "val_size = int(len(X)*VAL_PERCENT)\n",
        "\n",
        "train_X = X[:-val_size]\n",
        "train_y = y[:-val_size]\n",
        "test_X = X[-val_size:]\n",
        "test_y = y[-val_size:]\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward_pass(X,y,train = False):\n",
        "    if train:\n",
        "        net.zero_grad()\n",
        "    outputs = net(X)\n",
        "    matches = [torch.argmax(i,0) == torch.argmax(j,0) for i,j in zip(outputs,y)]\n",
        "    acc = matches.count(True)/len(matches)\n",
        "    loss = loss_function(outputs,y)\n",
        "\n",
        "    if train:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return acc, loss\n",
        "\n",
        "def test(size=32):\n",
        "    X, y = test_X[:size], test_y[:size]\n",
        "    val_acc, val_loss = forward_pass(X.view(-1,INPUT_SIZE).to(device), y.to(device).view(-1,2))\n",
        "    return val_acc, val_loss\n",
        "\n",
        "def train(net,model_name):\n",
        "    with open(model_log_filename,\"a+\") as f:\n",
        "        for epoch in range(EPOCHS):\n",
        "                for i in tqdm(range(0, len(train_X), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:100] ..for now just to dev\n",
        "                    batch_X = train_X[i:i+BATCH_SIZE].view(-1,INPUT_SIZE)\n",
        "                    batch_y = train_y[i:i+BATCH_SIZE].view(-1,2)\n",
        "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "                    acc, loss = forward_pass(batch_X, batch_y, train=True)\n",
        "                    if i % BATCH_SIZE == 0:\n",
        "                        val_acc, val_loss = test(size=BATCH_SIZE)\n",
        "                        f.write(f\"{model_name},{round(float(time.time()),3)},{round(float(acc),2)},{round(float(loss), 4)},{round(float(val_acc),2)},{round(float(val_loss),4)}\\n\")\n",
        "\n",
        "\n",
        "def create_acc_loss_graph(model_name):\n",
        "    contents = open(model_log_filename,\"r\").read().split(\"\\n\")\n",
        "\n",
        "    times = []\n",
        "    accuracies = []\n",
        "    losses = []\n",
        "\n",
        "    val_accs = []\n",
        "    val_losses = []\n",
        "\n",
        "    for c in contents:\n",
        "        if model_name in c:\n",
        "            name, timestamp, acc, loss, val_acc, val_loss = c.split(\",\")\n",
        "            times.append(timestamp)\n",
        "            accuracies.append(acc)\n",
        "            losses.append(loss)\n",
        "\n",
        "            val_accs.append(float(val_acc))\n",
        "            val_losses.append(float(val_loss))\n",
        "\n",
        "\n",
        "    fog = plt.figure()\n",
        "\n",
        "    ax1 = plt.subplot2grid((2,1),(0,0))\n",
        "    ax2 = plt.subplot2grid((2,1), (1,0), sharex=ax1)\n",
        "\n",
        "\n",
        "    ax1.plot(times, accuracies, label=\"acc\")\n",
        "    ax1.plot(times,losses, label=\"loss\")\n",
        "    ax1.legend(loc=2)\n",
        "    ax1.axes.get_xaxis().set_ticks([])\n",
        "    ax1.axes.get_yaxis().set_ticks([])\n",
        "    ax1.set_title(\"Testing Accuracy and Loss\")\n",
        "   \n",
        "    ax2.plot(times,val_accs, label=\"val_acc\")\n",
        "    ax2.plot(times,val_losses, label=\"val_loss\")\n",
        "    ax2.legend(loc=2)\n",
        "    ax2.set_title(\"Validation Accuracy and Loss\")\n",
        "    plt.show()\n",
        "    plt.savefig(f\"../logs/graphs/{model_name}.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 30/30 [00:00<00:00, 34.51it/s]\n100%|██████████| 30/30 [00:00<00:00, 40.57it/s]\n100%|██████████| 30/30 [00:00<00:00, 36.97it/s]\n100%|██████████| 30/30 [00:00<00:00, 36.67it/s]\n100%|██████████| 30/30 [00:00<00:00, 38.00it/s]\nHidden Layer size of  4\n"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (1,) and (150,)",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-23-6d3a3e2f2106>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Hidden Layer size of \"\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mlayer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mcreate_acc_loss_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-22-8dff16051121>\u001b[0m in \u001b[0;36mcreate_acc_loss_graph\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"acc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \"\"\"\n\u001b[0;32m   1645\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1646\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1647\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1648\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (150,)"
          ]
        }
      ],
      "source": [
        "layer_sizes = [4,16,32,64,128,256,512,1024,2048]\n",
        "\n",
        "for layer_size in layer_sizes:\n",
        "    net = Net(layer_size)\n",
        "    optimizer = optim.Adam(net.parameters(),lr = 0.0005)\n",
        "    model_name = f\"model-layersize{layer_size}-{int(time.time())}\"\n",
        "    train(net,model_name)\n",
        "    print(\"Hidden Layer size of \" ,layer_size)\n",
        "    create_acc_loss_graph(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python36864bitf498ff391bd141efa82a8b19ebf9c273",
      "display_name": "Python 3.6.8 64-bit"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}